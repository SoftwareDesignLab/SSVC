## extract_exploit_ref.py

This script downloads all of the CVE json files from NVD at url https://nvd.nist.gov/vuln/data-feeds#JSON_FEED, parses the json for data on the CVEs, makes http head requests to the urls containing the exploits and outputs two CSV files:
exploit_hosts_{TIMESTAMP}.csv	-	hostname, frequency, head request status
exploits_{TIMESTAMP}.csv		-	CVE id, url, head request status

### Usage:
python3 extract_exploit_ref.py

The variable N_THREADS can be modified in the script to change the number of threads used to make concurrent head requests

## parse_github_readme.py

This script parses the Github README.md file at url https://raw.githubusercontent.com/nomi-sec/PoC-in-GitHub/master/README.md and saves output to:
readme_links_{TIMESTAMP}.csv	-	CVE id, repository url

### Usage:
python3 parse_github_readme.py

## cxsecurity_parser_urls.py

This script goes through all of the exploit pages on Cxsecurity at url https://cxsecurity.com/exploit/ and saves output to:
cxsecurity_exploit_links_{TIMESTAMP}.csv		-	url

### Usage:
python3 cxsecurity_parser_urls.py

The variable N_PAGES can be modified to reflect the total number of pages containing Cxsecurity exploit links (currently 84)

### Requirements:
beautifulsoup4		-	version 4.11.1

## cxsecurity_parser_exploits.py

Cxsecurity limits the number of times one can make requests to its  website within a certain time period, and when that limit is reached, the user is met with a "too many redirects" error on any cxsecurity page. (100 requests per hour should be fine, but this can be changed with the N_REQUESTS variable) This script is made to make a certain number of requests at a time to urls in a file generated by cxsecurity_parser_urls.py. The line number of the last url read from the input file is printed to stdout, which can be used as the starting index for the next execution, and “done” is printed to stdout when the url file is fully read. The output is a CSV file with date, CVE, CWE, remote, local, author, url.

### Usage:
python3 cxsecurity_exploits.py [--index INDEX] urls_file data_file

--index INDEX		-	what line to start reading from urls_file,
                        default value: 0
urls_file   		-	file path to read urls from (should be output
                        of cxsecurity_parser_urls.py)
data_file   		-	file path to save output CSV

The variable N_REQUESTS can be modified to change the number of requests made per execution of the script (too many may result in getting the “too many redirects” error for some time)

### Requirements:
beautifulsoup4		-	version 4.11.1

## parse_packetstorm.py

This script downloads all exploit data from Packet Storm Security. The downloading is done in two phases. First, .tgz archive files of all exploits posted every year since 1999 and every month of the current year are downloaded from a user’s page at url https://packetstormsecurity.com/files/author/9331/. The exploits from the current month are downloaded from url https://packetstormsecurity.com/files/tags/exploit/. The all files are saved in the directory “./packetstorm_exploits/”. The .tgz files are extracted, and then all of the .txt files under the “./packetstorm_exploits” directory are searched and the output is saved in a CSV file:
packetstorm_exploits_{TIMESTAMP}.csv		-	author, vendor, date, title,
tested on, software, CVE

### Usage:
python3 parse_packetstorm.py [--skip]

--skip, -s	-	skips the downloading step (use if files are already
downloaded)

The N_THREADS variable can be modified to change the number of threads that scan the downloaded files concurrently

### Requirements:
beautifulsoup4		-	version 4.11.1

## parse_0day.py

This script crawls the 0day.today pages for remote, local, web app, dos, and shellcode exploits and saves output to:
0day_exploits_{TIMESTAMP}.csv		-	author, category, platform, risk, date
added, 0day-id, CVE, description

### Usage:
python3 parse_0day.py

The DAILY_REQS variable can be modified to change the average number of times the script makes a request per day. This affects the MIN_DELAY and MAX_DELAY variables, which are the lower and upper bounds of the random wait period. By default, they are set to 1/2 and 3/2 the number of seconds between requests to average DAILY_REQS requests per day, but these can be individually modified

### Requirements:
beautifulsoup4			-	version 4.11.1
selenium			-	version 4.10.0
undetected_chromedriver	-	version 3.5.0
