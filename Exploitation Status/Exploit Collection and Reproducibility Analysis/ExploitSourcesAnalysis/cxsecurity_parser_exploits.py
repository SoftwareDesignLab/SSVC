# Name: cxsecurity_parser.py
# Author: Mark Rumsey
# Date: 6/1/23
# Description: Reads 100 cxsecurity exploit links from input CSV and
# downloads data into output CSV.
#
# Note: Cxsecurity limits the number of times one can make requests to its 
# website within a certain time period, and when that limit is reached, the user 
# is met with a "too many redirects" error on any cxsecurity page. (100 requests 
# per hour should be fine, but this can be changed with the N_REQUESTS variable) 
# This script is made to make a certain number of requests at a time to urls in a
# file generated by cxsecurity_parser_urls.py. The line number of the last url
# read from the input file is printed to stdout, which can be used as the starting 
# index for the next execution.

import requests
from bs4 import BeautifulSoup
import csv
import time
import argparse

N_REQUESTS = 100    # number of requests to make per execution

def parse_exploit(url, exploit_page, writer):
        
    # download page content and parse html
    html = BeautifulSoup(exploit_page.content, 'html.parser')

    # find data about exploit on page
    table_data = html.find_all('div', class_='well well-sm')

    date = table_data[0].find('b').text.strip()
    author = table_data[1].find('b').text.strip()
    local = table_data[3].find('b').text.strip()
    remote = table_data[4].find('b').text.strip()
    cve = table_data[5].find('b').text.strip()
    cwe = table_data[6].find('b').text.strip()

    # write data to file
    writer.writerow([date, cve, cwe, remote, local, author, url.replace('issue', 'ascii')])
        
def main():
    
    # parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('urls_file', type=argparse.FileType('r'), help='file path containing urls for exploits')
    parser.add_argument('data_file', type=argparse.FileType('a+'), help='file path to save data')
    parser.add_argument('--index', '-i', type=int, default=0, help='place in url file to start reading data')

    args = parser.parse_args()

    # create writer object
    writer = csv.writer(args.data_file)

    # move file pointer to beginning of file to look for header
    args.data_file.seek(0)
    if len(args.data_file.readlines()) == 0:
        header = ['date', 'cve', 'cwe', 'remote', 'local', 'author', 'url']
        writer.writerow(header)

    # move file pointer back to end of file
    args.data_file.seek(0, 2)

    requests_count = 0

    # read urls from input file and parse pages
    for line in args.urls_file.readlines()[args.index:]:

        url = line.strip()

        # make request or break if too many redirects error
        try:
            exploit_page = requests.get(url)
        except requests.exceptions.TooManyRedirects:
            break

        parse_exploit(url, exploit_page, writer)
        time.sleep(2)

        # break if max number of requests reached
        requests_count += 1
        if requests_count >= N_REQUESTS:
            break

    # print number of urls read or "done" if all have been read
    if requests_count < N_REQUESTS:
        print('done')
    else:
        print(args.index+requests_count)

    args.urls_file.close()
    args.data_file.close()
    

if __name__ == '__main__':
    main()