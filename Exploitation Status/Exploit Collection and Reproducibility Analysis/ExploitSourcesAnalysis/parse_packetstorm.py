# Name: parse_packetstorm.py
# Author: Mark Rumsey
# Date: 6/5/23
# Description: Downloads exploit data from packetstormsecurity.com and saves into CSV file

import os
import requests
import csv
import re
import time
import tarfile
import concurrent.futures
import threading
import itertools
import argparse
from datetime import datetime
from bs4 import BeautifulSoup

N_THREADS = 20      # number of threads to search downloaded files

def download_files(dir):

    print('Downloading archives')

    # make request to first page of exploit archies
    url = 'https://packetstormsecurity.com/files/authors/9331/page1/'
    page = requests.get(url)

    # parse page
    html = BeautifulSoup(page.content, 'html.parser')

    # get number of pages
    max_page = int(html.find('input', id='page-max')['value'])
    print(f'Page 1 of {max_page}')

    time.sleep(100)

    # download tar files from first page
    download_archives(html, dir)

    # download tar files from rest of pages
    for page_num in range(2, max_page+1):
        
        url = re.sub('page\d*', f'page{page_num}', url)
        page = requests.get(url)
        print(f'Page {page_num} of {max_page}')
        time.sleep(100)
        html = BeautifulSoup(page.content, 'html.parser')
        download_archives(html, dir)

    print('Downloading recent exploits')

    # directory to save downloaded recent exploit files
    new_dir = './packetstorm_exploits/new/'

    # make request to first page of exploits
    url = 'https://packetstormsecurity.com/files/tags/exploit/page1/'
    page = requests.get(url)

    # parse page
    html = BeautifulSoup(page.content, 'html.parser')

    # get number of pages
    max_page = int(html.find('input', id='page-max')['value'])
    print(f'Page 1 of {max_page}')

    time.sleep(100)

    # download exploit files from first page
    is_done, month = download_exploits(html, new_dir)

    # download exploit files from rest of pages
    for page_num in range(2, max_page+1):

        if is_done:
            break

        url = re.sub('page\d*', f'page{page_num}', url)
        page = requests.get(url)
        print(f'Page {page_num} of {max_page}')
        time.sleep(100)
        html = BeautifulSoup(page.content, 'html.parser')
        is_done, month = download_exploits(html, new_dir, month)

def download_archives(html, dir):
    
    # get current year
    now = datetime.now()
    year = str(now.year)

    # create directory for downloading files
    if not os.path.exists(dir):
        os.makedirs(dir)

    links = html.find_all('dd', class_='act-links')

    for link in links:

        # get link to file from page
        download_link = link.find('a')['href']

        # check for correct pattern in link
        if match := re.search(f'/((1999|20\d\d)|{year}(0[1-9]|1[0-2]))-exploits.tgz', download_link):

            filepath = dir + match.group()[1:]

            if not os.path.isfile(filepath):

                download_link = 'https://packetstormsecurity.com' + download_link

                # request page with download link
                download_page = requests.get(download_link)
                time.sleep(100)

                # parse page for download link
                html = BeautifulSoup(download_page.content, 'html.parser')
                file = html.find('div', class_='file')
                direct_download = file.find('a')['href']

                # download file if not already saved
                print(f'Downloading {match.group()[1:]}')
                r = requests.get(direct_download)
                with open(filepath, 'wb') as f:
                    f.write(r.content)
                time.sleep(100)

def download_exploits(html, dir, month=None):

    # get current year
    now = datetime.now()
    year = str(now.year)

    # create directory for downloading files
    if not os.path.exists(dir):
        os.makedirs(dir)

    # get list of exploits from page
    exploits = html.find_all('dl', class_=['file', 'file first'])

    for exploit in exploits:

        # extract information from exploit entry
        author_page = exploit.find('dd', class_='refer').find('a')['href']
        download_link = exploit.find('dd', class_='act-links').find('a')['href']
        filename = re.search('[^/]+$', download_link).group()
        date = exploit.find('dd', class_='datetime').find('a')['href']

        # stop searching after getting latest month's data
        if author_page.endswith('/9331/') and re.search(f'/((1999|20[0-9]{2})|{year}(0[1-9]|1[0-2]))-exploits.tgz', download_link):
            month = re.search('\d{4}-(\d{2})-\d{2}', date).group(1)

        else:

            # check if latest month finished downloading
            if month and month != re.search('\d*-(\d*)-\d*', date).group(1):
                return True, month
            
            # download file if not already saved
            filepath = dir + filename

            if not os.path.isfile(filepath):

                download_link = 'https://packetstormsecurity.com' + download_link

                print(f'Downloading {filename}')
                r = requests.get(download_link)
                with open(filepath, 'wb') as f:
                    f.write(r.content)
                time.sleep(100)

    return False, month


def extract_files(dir):

    # iterate through files in directory
    for file in os.scandir(dir):

        subdir = f'{dir}/{file.name[:-4]}'

        # extract file if not already extracted
        if file.is_file() and file.name.endswith('.tgz') and not os.path.isdir(subdir):

            os.makedirs(subdir)
            tar = tarfile.open(file.path)
            tar.extractall(subdir)
            tar.close()

def search_files(dir, writer, lock):

    # iterate through directory recursively
    for root, dirs, files in os.walk(dir):

        # parse file if it is a .txt
        for file in files:         

            if file.endswith('.txt'):
                path = os.path.join(root, file)
                parse_exploit(path, writer, lock)

    print(f'Searching {dir} complete')


def parse_exploit(path, writer, lock):

    exploit = open(path, 'r')

    author = None
    vendor = None
    date = None
    title = None
    software = None
    tested = None
    cve = None

    # read file line-by-line
    try:
        lines = exploit.readlines()
        for line in lines:

            # search for matching pattern and save data if matching
            if author == None:
                author = re.search('author[^:]*:\s*([^\s].*)$', line, re.IGNORECASE)
                if author:
                    author = author.group(1)
            if vendor == None:
                vendor = re.search('vendor[^:]*:\s*([^\s].*)$', line, re.IGNORECASE)
                if vendor:
                    vendor = vendor.group(1)
            if date == None:
                date = re.search('date[^:]*:\s*([^\s].*)$', line, re.IGNORECASE)
                if date:
                    date = date.group(1)
            if title == None:
                title = re.search('title[^:]*:\s*([^\s].*)$', line, re.IGNORECASE)
                if title:
                    title = title.group(1)
            if software == None:
                software = re.search('software[^:]*:\s*([^\s].*)$', line, re.IGNORECASE)
                if software:
                    software = software.group(1)
            if tested == None:
                tested = re.search('tested[^:]*:\s*([^\s].*)$', line, re.IGNORECASE)
                if tested:
                    tested = tested.group(1)
            if cve == None:
                cve = re.search('cve[^:]*:\s*([^\s].*)$', line, re.IGNORECASE)
                if cve:
                    cve = cve.group(1)

    # some files do not have proper encoding to be read
    except UnicodeDecodeError:
        return

    # write data to CSV file
    lock.acquire()
    writer.writerow([author, vendor, date, title, tested, software, cve])
    lock.release()

    exploit.close()


def main():

    # parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('--skip', '-s', action='store_true', help='skips downloading files step and starts extracting/scanning')
    args = parser.parse_args()

    # directory to save downloaded tar files
    dir = './packetstorm_exploits/'

    # skip downloads if --skip flag
    if not args.skip:
        download_files(dir)
    elif args.skip and not os.path.exists(dir):
        print('Files must be downloaded first')
        return

    # extract tar files
    print('Extracting files')
    extract_files(dir)

    # for 2021 and 2022, there is an extra nested directory with tar files that need to be extracted
    for file in os.scandir(dir):    
        if file.is_dir():
            for file2 in os.scandir(file.path):
                if file2.name == file.name:
                    extract_files(file2.path)

    # create file with timestamp
    now = str(datetime.now()).replace(' ', '_')
    outfile = open(f'packetstorm_exploits_{now}.csv', 'w')
    writer = csv.writer(outfile)

    # write header to csv
    header = ['Author', 'Vendor', 'Date', 'Title', 'Tested On', 'Software', 'CVE']
    writer.writerow(header)

    print('Searching files')

    lock = threading.Lock()

    # get list of subdirectories in main directory
    subdirs = [file.path for file in os.scandir(dir) if file.is_dir()]

    # concurrently scan directories and parse files for data
    with concurrent.futures.ThreadPoolExecutor(max_workers=N_THREADS) as executor:
        executor.map(search_files, subdirs, itertools.repeat(writer), itertools.repeat(lock))

    outfile.close()

    print(f'Output saved to packetstorm_exploits_{now}.csv')

if __name__ == '__main__':
    main()