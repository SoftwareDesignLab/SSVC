# Name: random_urls.py
# Author: Mark Rumsey
# Date: 6/21/23
# Description: randomly selects urls from file with variety of domains

import csv
import random
import requests

DOMAINS_PATH = 'exploit_hosts_2023-05-29_14:13:02.315339.csv'   # path to csv with exploit domains
URLS_PATH = 'exploits_2023_05_29.csv'                           # path to csv with exploit urls
DATA_SIZE = 1000                                                # length of output
EXCLUDE_SPECIAL = False                                         # whether or not to exclude urls that end in .pdf, .png, .jpg, or .mp4

def main():

    domains = []
    urls = []
    rare_domains = []

    # open files
    domains_file = open(DOMAINS_PATH, 'r')    
    urls_file = open(URLS_PATH, 'r')
    outfile = open('urls_new.csv', 'w')
    writer = csv.writer(outfile)

    # create list of active domains
    reader = csv.reader(domains_file)
    for line in reader:
        if line[2] == '200':
            for _ in range(int(line[1])):
                domains.append(line[0])
            # rare domain if < 1% of active urls from there
            if int(line[1]) < 160:
                rare_domains.append(line[0])

    # create list of active urls
    reader = csv.reader(urls_file)
    for line in reader:
        if line[2] == '200':
            if EXCLUDE_SPECIAL and not line[1].endswith(('.pdf', '.png', '.jpg', '.jpeg', '.mp4')):
                urls.append(line[1])
            elif not EXCLUDE_SPECIAL:
                urls.append(line[1])

    n_urls = 0

    # loop until desired number of urls are obtained
    while n_urls < DATA_SIZE:

        if n_urls % 10 == 0:
            print(n_urls)

        # 1/100 chance to select uncommon domain
        if random.randint(1, 100) == 100:
            domain_index = random.randint(0, len(rare_domains)-1)
            domain_pool = rare_domains
            # get list of urls with domain
            urls_with_domain = [url for url in urls if domain_pool[domain_index] in url]

            # remove domain if no more good urls with domain
            if len(urls_with_domain) == 0:
                domain_pool.pop(domain_index)
                continue

        # otherwise randomly select normally
        else:
            domain_index = random.randint(0, len(domains)-1)
            domain_pool = domains
            urls_with_domain = urls

        # randomly select url from urls with domain
        url_index = random.randint(0, len(urls_with_domain)-1)

        empty = False

        # make request
        try:
            code = requests.get(urls_with_domain[url_index], timeout=10).status_code
        except:
            urls_with_domain.pop(url_index)
            code = 0
        
        while code != 200:

            # remove url if not working
            try:
                urls_with_domain.pop(url_index)

            # no working urls under domain
            except IndexError:
                empty = True
                break
            if len(urls_with_domain) == 0:
                empty = True
                break

            # randomly select url from urls with domain
            url_index = random.randint(0, len(urls_with_domain)-1)

            # make request
            try:
                code = requests.get(urls_with_domain[url_index], timeout=10).status_code
            except:
                urls_with_domain.pop(url_index)
                code = 0

        # remove domain if no more urls left
        if empty:
            domain_pool.pop(domain_index)
            continue

        # write url to file
        writer.writerow([urls_with_domain[url_index]])

        # remove url from list
        urls.remove(urls_with_domain[url_index])

        n_urls += 1

    domains_file.close()
    urls_file.close()
    outfile.close()

if __name__ == '__main__':
    main()