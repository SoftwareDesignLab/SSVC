# Name: compare_trees
# Author: Eva Gorzkiewicz
# Modified: 7/8/2023
# Description: compares the HTML structure of two webpages and finds identical nodes

from lxml import etree
import requests
import csv
from urllib.parse import urlparse
import datetime
import time
import json
from collections import defaultdict
import concurrent.futures
import logging
import os

# global dictionary to store cached ASTs
cached_asts = {}

# configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s", filename="script.log")

# get identical nodes
def construct_ast(url):
    if url in cached_asts:
        return cached_asts[url]
    else:
        try:
            response = requests.get(url, timeout=10)
            html = response.content
            parser = etree.HTMLParser()
            tree = etree.fromstring(html, parser)
            cached_asts[url] = tree
            return tree
        except requests.exceptions.RequestException as e:
            logging.error(f"Error occurred while fetching URL {url}: {str(e)}")
            return None

def compare_ast(node1, node2):
    if node1 is None or node2 is None:
        return False

    if node1.tag != node2.tag:
        return False

    if node1.text != node2.text:
        return False

    if node1.tail != node2.tail:
        return False

    if node1.attrib != node2.attrib:
        return False

    if len(node1) != len(node2):
        return False

    return all(compare_ast(child1, child2) for child1, child2 in zip(node1, node2))

def find_identical_nodes(node1, node2):
    identical_nodes = []

    if node1 is None or node2 is None:
        return identical_nodes

    if compare_ast(node1, node2):
        identical_nodes.append(node1)

    for child1, child2 in zip(node1, node2): # allows to be compared in pairs
        identical_nodes.extend(find_identical_nodes(child1, child2))

    return identical_nodes

# write identical nodes to json file (domain name of url, xpath queries of all identical nodes)
def process_batch(batch_urls, domain_to_xpaths):
    domains = {}

    for url in batch_urls:
        domain = get_domain(url)
        try:
            if domain not in domains:
                domains[domain] = construct_ast(url)
            else:
                ast = construct_ast(url)
                identical_nodes = find_identical_nodes(domains[domain], ast)
                if domain not in domain_to_xpaths:
                    domain_to_xpaths[domain] = set(get_xpath(node) for node in identical_nodes)
                else:
                    # only retain XPaths similar between all URLs   
                    domain_to_xpaths[domain] &= set(get_xpath(node) for node in identical_nodes)
        except Exception as e:
            logging.error(f"Error occurred for URL {url}: {str(e)}")

def process_csv(input_file, output_file):
    with open(input_file, 'r') as csv_file:
        reader = csv.DictReader(csv_file)

        domain_to_xpaths = defaultdict(list)

        batch_size = 100
        urls_batch = []
        start_time = time.time()
        total_time = 0

        for line_num, row in enumerate(reader, start=1):
            url = row['exploit_url']
            urls_batch.append(url)
            if len(urls_batch) == batch_size: 
                # process batch of URLs in parallel using threads
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    executor.submit(process_batch, urls_batch, domain_to_xpaths)
                urls_batch = []

            if line_num % 100 == 0:
                end_time = time.time()
                time_elapsed = end_time - start_time
                total_time += time_elapsed
                gm_total = time.gmtime(total_time)
                start_time = end_time  

                current_time = datetime.datetime.now().time()
                formatted_time = current_time.strftime("%I:%M:%S %p")

                print(f"{line_num} lines processed at {formatted_time} "
                      f"(total runtime: {gm_total.tm_hour}hr {gm_total.tm_min}min {gm_total.tm_sec}s)")

        # process remaining URLs in the last batch
        if urls_batch:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                executor.submit(process_batch, urls_batch, domain_to_xpaths)

        domain_to_xpaths = {domain: list(set(xpaths)) for domain, xpaths in domain_to_xpaths.items()}

        # write to json
        with open(output_file, 'w') as outf:
            json.dump(domain_to_xpaths, outf, indent=4)

def get_domain(url):
    parsed_url = urlparse(url)
    return parsed_url.netloc

def get_xpath(node):
    xpath = node.getroottree().getpath(node)
    return xpath

# example usage
input_csv = 'output.csv'

current_date = datetime.date.today().strftime("%Y-%m-%d")
output_json = f'xpath_queries_{current_date}.json'
process_csv(input_csv, output_json)
