# Name: simplify_html
# Author: Eva Gorzkiewicz
# Modified: 6/23/2023 - Mark Rumsey
# Description: This script aims to simplify the structure of an HTML document by removing unnecessary tags and inline elements.

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
import json
import os
from lxml import etree
from urllib.parse import urlparse
import traceback
import jsonlines

INLINE_TAGS = ['a', 'abbr', 'acronym', 'b', 'bdo', 'big', 'br', 'button', 'cite', 'code', 'dfn', 'em', 'i',
                'img', 'input', 'kbd', 'label', 'map', 'object', 'output', 'q', 'samp', 'script', 'select',
                'small', 'span', 'strong', 'sub', 'sup', 'textarea', 'time', 'tt', 'var'] 

REMOVE_TAGS = ['nav', 'header', 'menu', 'aside', 'footer', 'script', 'style']

REMOVE_DIVS = ['nav', 'header', 'menu', 'aside', 'footer', 'head', 'search', 'menu', 'navbar', 'navigation',
                'sidebar', 'foot', 'footer', 'bottom', 'pagination']

# TODO: add more/change?
KEYWORDS = ['CVE', 'CWE', 'dates', 'remote', 'local', 'risk', 'host information', 'proof of concept', 'poc',
            'installation', 'steps', 'reproduce', 'platform', 'code', 'impact', 'product', 'vulneerability',
            'vulnerable', 'component', 'version']

def make_request(url):

    # make 3 attempts to get 200 status code
    for _ in range(3):
        response = requests.get(url)
        if response.status_code == 200:
            break
        elif response.status_code != 503:
            raise RuntimeError(f'HTTP response is not 200 (actual status code = {response.status_code})')
        
    if response.status_code == 200:
        return response.text
    else:
        raise RuntimeError(f'Failed to retrieve HTML content from URL: {url}\nHTTP response is not 200 (actual status code = {response.status_code})')

def parse_html(html):
    """
    Parse an HTML code provided as input
    @return: a parsed HTML (i.e., the DOM object)
    """
    return BeautifulSoup(html, 'html.parser')

def simplify_tree(dom):
    """
    Simplify the tree, by keeping only the block-level elements,
    while removing the inline elements (but keeping its innerText).
    @return a simplified DOM object
    """

    # Create a copy of the DOM object to avoid modifying the original
    simplified_dom = BeautifulSoup(str(dom), 'html.parser') 

    # Remove unnecessary tags and contents
    for tag in simplified_dom.find_all(REMOVE_TAGS): 
        if not any(keyword.lower() in tag.text.lower() for keyword in KEYWORDS):
            tag.decompose() 

    for element in simplified_dom.find_all():
        if element and element.attrs:
            element_id = element.get('id')
            element_class = element.get('class')

            if element_id or element_class:

                # Check if div_id contains the specific word
                if element_id and any(re.search(r'.*{}\.*'.format(re.escape(word)), element_id) for word in REMOVE_DIVS):
                    if not any(keyword.lower() in element.text.lower() for keyword in KEYWORDS):
                        element.decompose()
                    # else:
                    #     print(f'Keyword detected: {element.text}')

                # Check if any div_class contains the specific word
                if element_class and any(any(re.search(r'.*{}\.*'.format(re.escape(word)), cls) for word in REMOVE_DIVS) for cls in element_class):
                    if not any(keyword.lower() in element.text.lower() for keyword in KEYWORDS):
                        element.decompose()

    # Remove inline tags but keep content 
    for tag in simplified_dom.find_all(INLINE_TAGS): 
        if tag.name in INLINE_TAGS:
            tag.insert_before(tag.get_text())
            tag.decompose()

    # Remove white space
    simplified_dom = BeautifulSoup(str(simplified_dom).strip(), 'html.parser') 

    return simplified_dom

def load_xpath_queries_from_json(json_file):
    # dictionary
    with open(json_file, 'r') as file:
        xpath_queries = json.load(file)
    return xpath_queries

def remove_nodes_from_json(initial_dom, domain, xpath_queries):

    parsed_dom = etree.HTML(initial_dom.encode())

    if domain in xpath_queries:
        xpath_query_list = xpath_queries[domain]
        # first pass
        flagged_nodes = set()
        for xpath_query in xpath_query_list:
            nodes = parsed_dom.xpath(xpath_query)
            flagged_nodes.update(nodes)
            
        # second pass
        for node in flagged_nodes:
            if node is not None:
                if node.text is not None and node.text.strip() and any(keyword.lower() in node.text.lower() for keyword in KEYWORDS):
                    continue  # Skip if node contains keywords in its text
                parent = node.getparent()
                if parent is not None: # check it is not the root
                    parent.remove(node)

    # Convert the modified etree.Element back to BeautifulSoup
    modified_dom = BeautifulSoup(etree.tostring(parsed_dom, encoding='unicode'), 'html.parser')

    return modified_dom

def remove_html_tags(dom):
    for tag in dom.find_all():
        tag.string = tag.get_text()
    return dom

def process_url(url, index, error_file, writer):
    domain = urlparse(url).netloc
    json_file = 'xpath_queries_2023-06-13.json' # Replace with path to JSON file
    
    xpath_queries = load_xpath_queries_from_json(json_file)

    try: 
        html_text = make_request(url)
    except Exception as e:
        error_message = traceback.format_exc()
        with open(error_file, 'a') as file:
            file.write(f"Error processing URL: {url}\n\n{error_message}")

        return

    # print(f"HTML Text: {html_text}")  # debugging purposes

    html_dom = parse_html(html_text)

    if domain not in xpath_queries:
        # Apply ONLY filtering and cleaning rules
        # Calling function on 
        filtered_html = simplify_tree(html_dom)
        final_output = remove_html_tags(filtered_html)

    else:
        # Remove identical nodes
        simplified_dom = remove_nodes_from_json(html_dom, domain, xpath_queries)
        # Apply filtering and cleaning rules
        filtered_html = simplify_tree(simplified_dom)
        final_output = remove_html_tags(filtered_html)
    
    writer.write({"text": str(final_output)})


if __name__ == "__main__":

    urls_file = 'urls.csv' # path to the file containing URLs, one per line 
    error_file = 'errors.txt'
    output_file = 'data_encoded.jsonl'

    with open(urls_file, 'r') as file:
        urls = file.readlines()
    
    file = open(output_file, 'w')
    writer = jsonlines.Writer(file)

    for i, url in enumerate(urls, start=1):
        print(i)
        url = url.strip()
        try:
            process_url(url, i, error_file, writer)
        except RecursionError:
            error_message = traceback.format_exc()
            with open(error_file, 'a') as file:
                file.write(f"Error processing URL: {url}\n\n{error_message}")

    file.close()
    writer.close()
