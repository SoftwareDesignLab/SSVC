import pandas as pd
from urllib.parse import urlparse
from Levenshtein import distance

# get domain name
def extract_domain(url):
    parsed_url = urlparse(url)
    return parsed_url.netloc

# load CSV into pandas df
df = pd.read_csv('exploits_2023_05_29.csv')

# create a new column for domain name
df['domain'] = df['exploit_url'].apply(extract_domain)

# drop duplicate URLs
df.drop_duplicates(subset='exploit_url', keep='first', inplace=True)

# create a dictionary to store distances for each domain
domain_distances = {}

# compute Levenshtein distance for each URL within the same domain
for _, row in df.iterrows():
    domain = row['domain']
    cve_id = row['CVE_id']
    url = row['exploit_url']
    status = row['status']

    if status == '200':  # Check if status code is 200
        distances = domain_distances.get(domain, [])
        distances.append((cve_id, url, status, distance(url, domain)))
        domain_distances[domain] = distances

# create a new DataFrame with the top URLs
top_urls = []
for domain, distances in domain_distances.items():
    distances.sort(key=lambda x: x[1], reverse=True)  # Sort by distance
    top_urls.extend(distances[:min(len(distances), 5)])

# create a new DataFrame with the top URLs
top_df = pd.DataFrame(top_urls, columns=['CVE_id', 'exploit_url', 'status', 'distance'])

top_df.to_csv('output.csv', index=False)
